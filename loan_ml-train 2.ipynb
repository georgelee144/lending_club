{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the best way to fill the nans. Should it be filled with 0s or with the average of the respective feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('lending_club_ml.csv')\n",
    "df.drop(['id'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating columns for features that had more than 2 outcomes\n",
    "# I have decided to use subgrade and not grade as these are suppose to be similar features but, subgrade is more granular\n",
    "\n",
    "df = pd.concat([df, pd.get_dummies(\n",
    "    df.sub_grade, prefix='sub_grade', drop_first=True)], axis=1)\n",
    "df = pd.concat([df, pd.get_dummies(df.home_ownership,\n",
    "                                   prefix='home_ownership', drop_first=True)], axis=1)\n",
    "df = pd.concat([df, pd.get_dummies(df.verification_status,\n",
    "                                   prefix='verification_status', drop_first=True)], axis=1)\n",
    "df = pd.concat(\n",
    "    [df, pd.get_dummies(df.purpose, prefix='purpose', drop_first=True)], axis=1)\n",
    "df = pd.concat([df, pd.get_dummies(df.verification_status_joint,\n",
    "                                   prefix='verification_status_joint', drop_first=True)], axis=1)\n",
    "\n",
    "df.drop(columns=['sub_grade', 'home_ownership', 'verification_status',\n",
    "                 'purpose', 'verification_status_joint'], inplace=True)\n",
    "\n",
    "\n",
    "# conversions for features that only had 2 outcomes\n",
    "\n",
    "df.disbursement_method = df.disbursement_method.apply(\n",
    "    lambda disburstment: 1 if disburstment == 'Cash' else 0)\n",
    "\n",
    "df.application_type = df.application_type.apply(\n",
    "    lambda app_type: 1 if app_type == 'Individual' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all0 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2506738 entries, 0 to 2506737\n",
      "Columns: 148 entries, loan_amnt to verification_status_joint_Verified\n",
      "dtypes: float64(89), int64(5), object(1), uint8(53)\n",
      "memory usage: 1.9+ GB\n"
     ]
    }
   ],
   "source": [
    "# if borrower is an indivudal then their secondary features are filled with their single counterpart\n",
    "\n",
    "sec_list = ['sec_app_fico_range_low', 'sec_app_fico_range_high', 'sec_app_inq_last_6mths', 'sec_app_mort_acc',\n",
    "            'sec_app_open_acc', 'sec_app_revol_util', 'sec_app_open_act_il','sec_app_num_rev_accts', 'sec_app_chargeoff_within_12_mths',\n",
    "            'sec_app_collections_12_mths_ex_med', 'sec_app_mths_since_last_major_derog']\n",
    "joint_list = ['dti_joint', 'revol_bal_joint', 'annual_inc_joint']\n",
    "\n",
    "for secondary in sec_list:\n",
    "    df.loc[df.application_type == 1,secondary] = df.loc[df.application_type == 1][secondary[8:]]\n",
    "\n",
    "\n",
    "for joint in joint_list:\n",
    "    df.loc[df.application_type == 1,joint] = df.loc[df.application_type == 1][joint[:len(joint)-6]]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill all nans with averages of respective feature\n",
    "\n",
    "df_global = df.copy()\n",
    "\n",
    "for thing in df_global:\n",
    "    if df_global[thing].isnull().values.any():\n",
    "        df_global[thing].fillna(np.mean(df_global[thing]), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill all nans with 0\n",
    "df_global.fillna(0, inplace=True)\n",
    "df_all0.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def scoring(clf, x, y):\n",
    "    \n",
    "    #Baseline\n",
    "    print('Loan passing rate:', np.mean(y))\n",
    "    print('Balanced loan passing rate:', np.average(y, weights=x['loan_amnt']))\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "    print('score: ', clf.score(x, y))\n",
    "    # score adjusted for loan amount\n",
    "    print('balanced_accuracy_score: ', balanced_accuracy_score(\n",
    "        y, clf.predict(x), sample_weight=x['loan_amnt']))\n",
    "\n",
    "    print(confusion_matrix(y, clf.predict(x)))\n",
    "    \n",
    "    print('F1 score: ', f1_score(y, clf.predict(x)))\n",
    "    \n",
    "    print('precision_score: ', precision_score(y, clf.predict(x)))\n",
    "\n",
    "    # score adjusted for loan amount\n",
    "    print('average_precision_score: ', average_precision_score(\n",
    "        y, clf.predict(x), average='weighted', sample_weight=x['loan_amnt']))\n",
    "    \n",
    "    print('recall_score: ', recall_score(y, clf.predict(x)))\n",
    "\n",
    "    print('roc: ', roc_auc_score(y, clf.predict_proba(x)[:, 1]))\n",
    "\n",
    "    # score adjusted for loan amount\n",
    "    print('roc_weighted: ', roc_auc_score(y, clf.predict_proba(x)[\n",
    "          :, 1], average='weighted', sample_weight=x['loan_amnt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan passing rate: 0.8009672589365736\n",
      "Balanced loan passing rate: 0.7855872316459166\n",
      "\n",
      "\n",
      "score:  0.941541603836058\n",
      "balanced_accuracy_score:  0.8602047804170503\n",
      "[[108065  41612]\n",
      " [  2350 599995]]\n",
      "F1 score:  0.9646594080800546\n",
      "precision_score:  0.9351440991136318\n",
      "average_precision_score:  0.9292314399682938\n",
      "recall_score:  0.9960985813777818\n",
      "roc:  0.9644059362464467\n",
      "roc_weighted:  0.9652058267062085\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_global.drop(['grade','loan_status'], axis=1)\n",
    "y = df_global.loan_status\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf_best = RandomForestClassifier(criterion= 'entropy', n_estimators= 101)\n",
    "rf_best.fit(X_train, y_train)\n",
    "    \n",
    "scoring(rf_best, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST\n",
      "Loan passing rate: 0.8009672589365736\n",
      "Balanced loan passing rate: 0.7855872316459166\n",
      "\n",
      "\n",
      "score:  0.9415562310677081\n",
      "balanced_accuracy_score:  0.8601988309706627\n",
      "[[108010  41667]\n",
      " [  2284 600061]]\n",
      "F1 score:  0.9646716872723706\n",
      "precision_score:  0.935070621821083\n",
      "average_precision_score:  0.9292277605564438\n",
      "recall_score:  0.9962081531348314\n",
      "roc:  0.9647164436635872\n",
      "roc_weighted:  0.9655456417876066\n"
     ]
    }
   ],
   "source": [
    "X0 = df_all0.drop(['grade','loan_status'], axis=1)\n",
    "y0 = df_all0.loan_status\n",
    "\n",
    "X0_train, X0_test, y0_train, y0_test = train_test_split(X0, y0, test_size=0.3, random_state=42)\n",
    "\n",
    "rf0_best = RandomForestClassifier(criterion= 'entropy', n_estimators= 101)\n",
    "rf0_best.fit(X0_train, y0_train)\n",
    "    \n",
    "print('RANDOM FOREST')\n",
    "\n",
    "scoring(rf0_best, X0_test, y0_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loan passing rate: 0.8009672589365736\n",
      "Balanced loan passing rate: 0.7855872316459166\n",
      "\n",
      "\n",
      "score:  0.8079324806987029\n",
      "balanced_accuracy_score:  0.5943855683977545\n",
      "[[ 31680 117997]\n",
      " [ 26442 575903]]\n",
      "F1 score:  0.8885712191753873\n",
      "precision_score:  0.8299510015852428\n",
      "average_precision_score:  0.8188008540209637\n",
      "recall_score:  0.9561015696984286\n",
      "roc:  0.739744197418488\n",
      "roc_weighted:  0.7478246696409757\n",
      "Loan passing rate: 0.8009672589365736\n",
      "Balanced loan passing rate: 0.7855872316459166\n",
      "\n",
      "\n",
      "score:  0.8570480650832023\n",
      "balanced_accuracy_score:  0.6510727436734819\n",
      "[[ 45088 104589]\n",
      " [  2914 599431]]\n",
      "F1 score:  0.917708297451325\n",
      "precision_score:  0.8514402999914775\n",
      "average_precision_score:  0.8400250971028239\n",
      "recall_score:  0.9951622409084495\n",
      "roc:  0.8543565033812119\n",
      "roc_weighted:  0.8647971196425178\n"
     ]
    }
   ],
   "source": [
    "scoring(rf_best, X0_test, y0_test)\n",
    "scoring(rf0_best, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided to go with filling the nans with 0s. The classifier trained on the 0 seems to score better across both sets. Another reason I have decided to got with zero is because LC has been in the works of changing how the screen loan applicants. This may greatly skew the average."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
